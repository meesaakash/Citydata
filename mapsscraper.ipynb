{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934c47e-a073-4f5d-8328-d97fcd942de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test chromedriver\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()  # Optional argument, if not specified will search path.\n",
    "\n",
    "driver.get('http://www.google.com/');\n",
    "\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "\n",
    "search_box = driver.find_element_by_name('q')\n",
    "\n",
    "search_box.send_keys('ChromeDriver')\n",
    "\n",
    "search_box.submit()\n",
    "\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaca19a-46dc-4fd5-b729-9c3e9662d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from gaspa93 github to test with. \n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0a875c-c9ec-44e6-b00d-c664288af31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7027352-ae68-419b-8ac0-074fc486f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_WEBPAGE = 'https://www.google.com/maps/'\n",
    "MAX_WAIT = 10\n",
    "MAX_RETRY = 5\n",
    "MAX_SCROLLS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681dcae7-59cc-46ed-89c5-0e578a627393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleMapsScraper:\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        self.driver = self.__get_driver()\n",
    "        self.logger = self.__get_logger()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        if exc_type is not None:\n",
    "            traceback.print_exception(exc_type, exc_value, tb)\n",
    "\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def sort_by(self, url, ind):\n",
    "\n",
    "        self.driver.get(url)\n",
    "        self.__click_on_cookie_agreement()\n",
    "\n",
    "        wait = WebDriverWait(self.driver, MAX_WAIT)\n",
    "\n",
    "        # open dropdown menu\n",
    "        clicked = False\n",
    "        tries = 0\n",
    "        while not clicked and tries < MAX_RETRY:\n",
    "            try:\n",
    "                menu_bt = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@data-value=\\'Sort\\']')))\n",
    "                menu_bt.click()\n",
    "\n",
    "                clicked = True\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                tries += 1\n",
    "                self.logger.warn('Failed to click sorting button')\n",
    "\n",
    "            # failed to open the dropdown\n",
    "            if tries == MAX_RETRY:\n",
    "                return -1\n",
    "\n",
    "        #  element of the list specified according to ind\n",
    "        recent_rating_bt = self.driver.find_elements_by_xpath('//div[@role=\\'menuitemradio\\']')[ind]\n",
    "        recent_rating_bt.click()\n",
    "\n",
    "        # wait to load review (ajax call)\n",
    "        time.sleep(5)\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def get_places(self, keyword_list=None):\n",
    "\n",
    "        df_places = pd.DataFrame()\n",
    "        search_point_url_list = self._gen_search_points_from_square(keyword_list=keyword_list)\n",
    "\n",
    "        for i, search_point_url in enumerate(search_point_url_list):\n",
    "            print(search_point_url)\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"{i}/{len(search_point_url_list)}\")\n",
    "                df_places = df_places[['search_point_url', 'href', 'name', 'rating', 'num_reviews', 'close_time', 'other']]\n",
    "                df_places.to_csv('output/places_wax.csv', index=False)\n",
    "\n",
    "\n",
    "            try:\n",
    "                self.driver.get(search_point_url)\n",
    "            except NoSuchElementException:\n",
    "                self.driver.quit()\n",
    "                self.driver = self.__get_driver()\n",
    "                self.driver.get(search_point_url)\n",
    "\n",
    "            # scroll to load all (20) places into the page\n",
    "            scrollable_div = self.driver.find_element_by_css_selector(\n",
    "                \"div.m6QErb.DxyBCb.kA9KIf.dS8AEf.ecceSd > div[aria-label*='Results for']\")\n",
    "            for i in range(10):\n",
    "                self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "            # Get places names and href\n",
    "            time.sleep(2)\n",
    "            response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            div_places = response.select('div[jsaction] > a[href]')\n",
    "\n",
    "            for div_place in div_places:\n",
    "                place_info = {\n",
    "                    'search_point_url': search_point_url.replace('https://www.google.com/maps/search/', ''),\n",
    "                    'href': div_place['href'],\n",
    "                    'name': div_place['aria-label']\n",
    "                }\n",
    "\n",
    "                df_places = df_places.append(place_info, ignore_index=True)\n",
    "\n",
    "            # TODO: implement click to handle > 20 places\n",
    "\n",
    "        df_places = df_places[['search_point_url', 'href', 'name']]\n",
    "        df_places.to_csv('output/places_wax.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "    def get_reviews(self, offset):\n",
    "\n",
    "        # scroll to load reviews\n",
    "        self.__scroll()\n",
    "\n",
    "        # wait for other reviews to load (ajax)\n",
    "        time.sleep(4)\n",
    "\n",
    "        # expand review text\n",
    "        self.__expand_reviews()\n",
    "\n",
    "        # parse reviews\n",
    "        response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        # TODO: Subject to changes\n",
    "        rblock = response.find_all('div', class_='jftiEf fontBodyMedium ')\n",
    "        parsed_reviews = []\n",
    "        for index, review in enumerate(rblock):\n",
    "            if index >= offset:\n",
    "                r = self.__parse(review)\n",
    "                parsed_reviews.append(r)\n",
    "\n",
    "                # logging to std out\n",
    "                print(r)\n",
    "\n",
    "        return parsed_reviews\n",
    "\n",
    "\n",
    "\n",
    "    # need to use different url wrt reviews one to have all info\n",
    "    def get_account(self, url):\n",
    "\n",
    "        self.driver.get(url)\n",
    "        self.__click_on_cookie_agreement()\n",
    "\n",
    "        # ajax call also for this section\n",
    "        time.sleep(2)\n",
    "\n",
    "        resp = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "        place_data = self.__parse_place(resp, url)\n",
    "\n",
    "        return place_data\n",
    "\n",
    "\n",
    "    def __parse(self, review):\n",
    "\n",
    "        item = {}\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            id_review = review['data-review-id']\n",
    "        except Exception as e:\n",
    "            id_review = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            username = review['aria-label']\n",
    "        except Exception as e:\n",
    "            username = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            review_text = self.__filter_string(review.find('span', class_='wiI7pd').text)\n",
    "        except Exception as e:\n",
    "            review_text = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            rating = float(review.find('span', class_='kvMYJc')['aria-label'].split(' ')[0])\n",
    "        except Exception as e:\n",
    "            rating = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            relative_date = review.find('span', class_='rsqaWe').text\n",
    "        except Exception as e:\n",
    "            relative_date = None\n",
    "\n",
    "        try:\n",
    "            n_reviews = review.find('div', class_='RfnDt').text.split(' ')[3]\n",
    "        except Exception as e:\n",
    "            n_reviews = 0\n",
    "\n",
    "        try:\n",
    "            user_url = review.find('button', class_='WEBjve')['data-href']\n",
    "        except Exception as e:\n",
    "            user_url = None\n",
    "\n",
    "        item['id_review'] = id_review\n",
    "        item['caption'] = review_text\n",
    "\n",
    "        # depends on language, which depends on geolocation defined by Google Maps\n",
    "        # custom mapping to transform into date should be implemented\n",
    "        item['relative_date'] = relative_date\n",
    "\n",
    "        # store datetime of scraping and apply further processing to calculate\n",
    "        # correct date as retrieval_date - time(relative_date)\n",
    "        item['retrieval_date'] = datetime.now()\n",
    "        item['rating'] = rating\n",
    "        item['username'] = username\n",
    "        item['n_review_user'] = n_reviews\n",
    "        #item['n_photo_user'] = n_photos  ## not available anymore\n",
    "        item['url_user'] = user_url\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __parse_place(self, response, url):\n",
    "\n",
    "        place = {}\n",
    "\n",
    "        try:\n",
    "            place['name'] = response.find('h1', class_='DUwDvf fontHeadlineLarge').text.strip()\n",
    "        except Exception as e:\n",
    "            place['name'] = None\n",
    "\n",
    "        try:\n",
    "            place['overall_rating'] = float(response.find('div', class_='F7nice ').find('span', class_='ceNzKf')['aria-label'].split(' ')[1])\n",
    "        except Exception as e:\n",
    "            place['overall_rating'] = None\n",
    "\n",
    "        try:\n",
    "            place['n_reviews'] = int(response.find('div', class_='F7nice ').text.split('(')[1].replace(',', '').replace(')', ''))\n",
    "        except Exception as e:\n",
    "            place['n_reviews'] = 0\n",
    "\n",
    "        try:\n",
    "            place['n_photos'] = int(response.find('div', class_='YkuOqf').text.replace('.', '').replace(',','').split(' ')[0])\n",
    "        except Exception as e:\n",
    "            place['n_photos'] = 0\n",
    "\n",
    "        try:\n",
    "            place['category'] = response.find('button', jsaction='pane.rating.category').text.strip()\n",
    "        except Exception as e:\n",
    "            place['category'] = None\n",
    "\n",
    "        try:\n",
    "            place['description'] = response.find('div', class_='PYvSYb').text.strip()\n",
    "        except Exception as e:\n",
    "            place['description'] = None\n",
    "\n",
    "        b_list = response.find_all('div', class_='Io6YTe fontBodyMedium')\n",
    "        try:\n",
    "            place['address'] = b_list[0].text\n",
    "        except Exception as e:\n",
    "            place['address'] = None\n",
    "\n",
    "        try:\n",
    "            place['website'] = b_list[1].text\n",
    "        except Exception as e:\n",
    "            place['website'] = None\n",
    "\n",
    "        try:\n",
    "            place['phone_number'] = b_list[2].text\n",
    "        except Exception as e:\n",
    "            place['phone_number'] = None\n",
    "    \n",
    "        try:\n",
    "            place['plus_code'] = b_list[3].text\n",
    "        except Exception as e:\n",
    "            place['plus_code'] = None\n",
    "\n",
    "        try:\n",
    "            place['opening_hours'] = response.find('div', class_='t39EBf GUrTXd')['aria-label'].replace('\\u202f', ' ')\n",
    "        except:\n",
    "            place['opening_hours'] = None\n",
    "\n",
    "        place['url'] = url\n",
    "\n",
    "        lat, long, z = url.split('/')[6].split(',')\n",
    "        place['lat'] = lat[1:]\n",
    "        place['long'] = long\n",
    "\n",
    "        return place\n",
    "\n",
    "\n",
    "    def _gen_search_points_from_square(self, keyword_list=None):\n",
    "        # TODO: Generate search points from corners of square\n",
    "\n",
    "        keyword_list = [] if keyword_list is None else keyword_list\n",
    "\n",
    "        square_points = pd.read_csv('input/square_points.csv')\n",
    "\n",
    "        cities = square_points['city'].unique()\n",
    "\n",
    "        search_urls = []\n",
    "\n",
    "        for city in cities:\n",
    "\n",
    "            df_aux = square_points[square_points['city'] == city]\n",
    "            latitudes = df_aux['latitude'].unique()\n",
    "            longitudes = df_aux['longitude'].unique()\n",
    "            coordinates_list = list(itertools.product(latitudes, longitudes, keyword_list))\n",
    "\n",
    "            search_urls += [f\"https://www.google.com/maps/search/{coordinates[2]}/@{str(coordinates[1])},{str(coordinates[0])},{str(15)}z\"\n",
    "             for coordinates in coordinates_list]\n",
    "\n",
    "        return search_urls\n",
    "\n",
    "\n",
    "    # expand review description\n",
    "    def __expand_reviews(self):\n",
    "        # use XPath to load complete reviews\n",
    "        # TODO: Subject to changes\n",
    "        links = self.driver.find_elements_by_xpath('//button[@jsaction=\"pane.review.expandReview\"]')\n",
    "        for l in links:\n",
    "            l.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    def __scroll(self):\n",
    "        # TODO: Subject to changes\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "        #self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "    def __get_logger(self):\n",
    "        # create logger\n",
    "        logger = logging.getLogger('googlemaps-scraper')\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler and set level to debug\n",
    "        fh = logging.FileHandler('gm-scraper.log')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # add formatter to ch\n",
    "        fh.setFormatter(formatter)\n",
    "\n",
    "        # add ch to logger\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "    def __get_driver(self, debug=False):\n",
    "        options = Options()\n",
    "\n",
    "        if not self.debug:\n",
    "            options.add_argument(\"--headless\")\n",
    "        else:\n",
    "            options.add_argument(\"--window-size=1366,768\")\n",
    "\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        #options.add_argument(\"--lang=en-GB\")\n",
    "        options.add_argument(\"--accept-lang=en-GB\")\n",
    "        input_driver = webdriver.Chrome(executable_path=ChromeDriverManager(log_level=0).install(), options=options)\n",
    "\n",
    "         # click on google agree button so we can continue (not needed anymore)\n",
    "         # EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"I agree\")]')))\n",
    "        input_driver.get(GM_WEBPAGE)\n",
    "\n",
    "        return input_driver\n",
    "\n",
    "    # cookies agreement click\n",
    "    def __click_on_cookie_agreement(self):\n",
    "        try:\n",
    "            agree = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"Reject all\")]')))\n",
    "            agree.click()\n",
    "\n",
    "            # back to the main page\n",
    "            # self.driver.switch_to_default_content()\n",
    "\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # util function to clean special characters\n",
    "    def __filter_string(self, str):\n",
    "        strOut = str.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        return strOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d877e-fe61-4064-bca9-6f5a8c8b639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b87e5f-fe08-4d93-8243-ca6865da796d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
