import requests
import json

# 'X-Goog-FieldMask': 'places.displayName,places.formattedAddress,places.priceLevel'
# 'X-Goog-FieldMask': 'places.displayName,places.rating,places.formattedAddress,places.types,places.viewport'

def textsearchrequest():
    # should remove key when publishing to github
    myapikey = 'AIzaSyAHpf_Qy97E-SqC5GhEQqYIJrdB11IqkPg'
    url = 'https://places.googleapis.com/v1/places:searchText'

    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': myapikey,
        'X-Goog-FieldMask': '*'
    }
    data = {
        'textQuery': 'whole foods',
        'maxResultCount': 5,
        'locationBias': {
            'circle': {
                'center': {
                    'latitude': 40.764917,
                    'longitude': -73.9897431
                },
                'radius': 1000.0   
                }
            }
    }
    # want to create a function that can create a locationbias based on a latitude and longitude created from a search of a location (ex. Upper East Side 72d st)
    # input the long and lat values into the data var here before calling the request

    
    response = requests.post(url, json=data, headers=headers)
    data = json.loads(response.text)
    print(data)
    return data

test = textsearchrequest()



myfirst = test['places'][0:3]
myfirst


def findlocationarea(prompt):
    myapikey = 'AIzaSyAHpf_Qy97E-SqC5GhEQqYIJrdB11IqkPg'
    url = 'https://places.googleapis.com/v1/places:searchText'
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': myapikey,
        'X-Goog-FieldMask': 'places.displayName,places.rating,places.formattedAddress,places.viewport'
    }
    data = {
        'textQuery': 'Gyms near 72nd st in upper east side manhattan',
        'maxResultCount': 5
    }


import csv

def getlocationsfromcsv():
    myfile = 'DOITT_SUBWAY_STATION_01_13SEPT2010.csv'
    rows=[]
    with open(myfile, 'r') as file:
        csvreader = csv.reader(file)
        header = next(csvreader)
        for row in csvreader:
            rows.append(row[2:4])
            # rows.append(row[4])
    # print(header)
    print(rows)
    return rows

aasdfh = getlocationsfromcsv()

    



# Method will take each location listed in the csv and parse latitude and longitude from it
# Then requests a search of shops in that location near the coordinates, based on the search param and radius
def findspotsnearlocation():
    # location = getlocationsfromcsv()
    location = aasdfh
    # print(len(location)) 
    # len is 473 stations
    counter = 0 
    while counter<10:
        # print(location[counter][1])
        stationname = location[counter][0]
        latlong = location[counter][1].split(" ")
        longitude = float(latlong[1][1:])
        latitude= float(latlong[2][:-1])
        print("Station " + stationname + " is at: " + str(latitude) + ", " + str(longitude))
        searchparam = "grocery stores"
        radius = 5.0
        search = locationtextsearchrequest(stationname, latitude, longitude, radius, searchparam)
        counter = counter+1
    
    return search


test = findspotsnearlocation()
    


import requests
import json

# This method sends the api requet to the Google Places API with my independent key

def locationtextsearchrequest(station: str, lat: float, long: float, radius: float, txtquery: str):
    # should remove key when publishing to github
    myapikey = 'AIzaSyAHpf_Qy97E-SqC5GhEQqYIJrdB11IqkPg'
    url = 'https://places.googleapis.com/v1/places:searchText'
        # 'X-Goog-FieldMask': 'places.displayName,places.rating,places.formattedAddress,places.types,places.viewport'

    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': myapikey,
        'X-Goog-FieldMask': '*'

    }
    data = {
        'textQuery': txtquery,
        'maxResultCount': 1,
        'locationBias': {
            'circle': {
                'center': {
                    'latitude': lat,
                    'longitude': long
                },
                'radius': radius   
                }
            }
    }
    # want to create a function that can create a locationbias based on a latitude and longitude created from a search of a location (ex. Upper East Side 72d st)
    # input the long and lat values into the data var here before calling the request

    
    response = requests.post(url, json=data, headers=headers)
    data = json.loads(response.text)
    print(data)
    return data



# GOals:
 # get the different metrics you can extract out of reviews
# look through each of the reviews at a grocery store (e.g. whole foods, 
# and find out the overall impressions from the customer reveiews"
test


import json
# sort through the big json and pull out the info we need
# want an array of all the reviews, so then can use a NLP at later step to analyze arguments and determine sentiment
# want the avg rating and avg price point
# pull name, and location in city


# test

def sortjson(jsonfile):
    
    # probably should have try statements for each of these fields incase of error
    myfile = jsonfile['places'][0]
    name = myfile['displayName']['text']
    websiteurl = myfile['websiteUri']
    # currently pulls 5 reviews, need to modify it to get more reviews or reviews sorted by some param
    reviewslist = myfile['reviews']
    pricelevel = 'N/A'
    try: 
        pricelevel = myfile['priceLevel']
    except:
        pass
    # print(reviewslist)
    ret=[name, websiteurl, pricelevel, reviewslist]
    return ret

mysortedjson = sortjson(test)
mysortedjson


# Descripton of Roots:
# A ethnic indian chai flavored protein powder with organic ingredients targeted for weightlifters interested in diverse flavors 
# and cultural heritage
# * add desc of where we want to sell roots, target markets and size 
# see if nlp model can compare this claim to the ai models


input = "A food market with quality food products of diverse ethnic backgrounds "
# Hugging faces inferences api 
# https://huggingface.co/ProsusAI/finbert?text=Stocks+rallied+and+the+British+pound+gained.
# FinBERT is a pre-trained NLP model to analyze sentiment of financial text

# Summarize  like 10 reviews together if possible with one of the api's then for a series of locations summaries run the below query 
# find out which regions have the highest scores compared to the source sentence

import requests

API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
headers = {"Authorization": "Bearer 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"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()
	
output = query({
	"inputs": {
		"source_sentence": "That is a happy person",
		"sentences": [
			"That is a happy dog",
			"That is a very happy person",
			"Today is a sunny day"
		]
	},
})

output



