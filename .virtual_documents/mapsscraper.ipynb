# # test chromedriver
# import time

# from selenium import webdriver



# driver = webdriver.Chrome()  # Optional argument, if not specified will search path.

# driver.get('http://www.google.com/');

# time.sleep(5) # Let the user actually see something!

# search_box = driver.find_element_by_name('q')

# search_box.send_keys('ChromeDriver')

# search_box.submit()

# time.sleep(5) # Let the user actually see something!

# driver.quit()


from googlemaps import GoogleMapsScraper
from datetime import datetime, timedelta
import argparse
import csv
from termcolor import colored
import time

ind = {'most_relevant' : 0, 'newest': 1, 'highest_rating': 2, 'lowest_rating': 3}
HEADER = ['id_review','caption','relative_date','retrieval_date','rating','username','n_review_user','n_photo_user','url_user']
HEADER_W_SOURCE=['id_review','caption','relative_date','retrieval_date','rating','username','n_review_user','n_photo_user','n_photo_user','url_user','url_source']

def csv_writer(source_field, ind_sort_by, path='data/'):
    outfile= ind_sort_by + '_gm_reviews.csv'
    targetfile = open(path+outfile, mode='w', encoding='utf-8',newline='\n')
    writer=csv.writer(targetfile,quoting=csv.QUOTE_MINIMAL)

    if source_field:
        h= HEADER_W_SOURCE
    else:
        h = HEADER
    writer.writerow(h)
    return writer


ind['highest_rating']


path = '/Users/akashmeesa/desktop/citydata'
var_to_see_metadata= False
num_args = 20
want_source = False
sort_by= 'highest_rating'

def scrape(filename):

    writer = csv_writer(want_source, sort_by)
    
    with GoogleMapsScraper() as scraper:
        with open(filename,'r') as urls_file:
            for url in urls_file:
                # add way to check if place is false if i want metadata instead of reviews, probably want this. 
                if var_to_see_metadata==True:
                    print(scraper.get_account(url))
                else:
                    print(scraper.get_account(url))
                    error = scraper.sort_by(url, ind[sort_by])
                    if error == 0:
                        n = 0
                        while n< num_args:
                            print('[Review ' + str(n) + ']')
                            reviews = scraper.get_reviews(n)
                            if len(reviews)==0:
                                print('len reviews is 0')
                                break
                            for r in reviews:
                                row_data = list(r.values)
                                if want_source:
                                    row_data.append(url[:-1])
                                writer.writerow(row_data)
                        n += len(reviews)
    return "done"
                                
HEADER
                


ret = scrape('urls.txt')
ret



